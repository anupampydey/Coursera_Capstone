{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python_defaultSpec_1601061653497",
   "display_name": "Python 3.8.3 64-bit"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "## DATA FOR MODELLING\n",
    "Our aim is to prepare a model that will predict the severity of car accidents in the city of Seattle for a set of conditions. In other words, given the weather and the road conditions what is the possibility of a commuter getting into a car accident and how severe it would be. \n",
    "So lets analyse our data which is available at the IBM cloud storage -<br>\n",
    "cloudpath = \"https://s3.us.cloud-object-storage.appdomain.cloud/cf-courses-data/CognitiveClass/DP0701EN/version-2/Data-Collisions.csv\" <br>\n",
    "The source of this dataset is SDOT Traffic collisions report provided by Seattle Police Department. This dataset includes all type of collisions which happened in SDOT Traffic Division from 2004 to present. The dataset consists of 37 features (or columns) and 194763 entries (or rows). To analyse the data we need to carry out Data Wrangling or cleaning. \n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "~~~python\n",
    "# import the necessary libraries\n",
    "import pandas as pd \n",
    "import numpy as np \n",
    "print('Pandas version =', pd.__version__)\n",
    "print('Numpy version =', np.__version__)\n",
    "# Reading the csv file into pandas dataframe\n",
    "collision_df = pd.read_csv(\"Data-Collisions.csv\")\n",
    "print(\"csv file successfully read in dataframe\")\n",
    "collision_df.head() # display the first 5 rows of dataframe"
   ],
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   }
  },
  {
   "source": [
    "After analysing the dataset, we can see that **SeverityCode** is our target or dependent variable.<br>\n",
    "Now we need to check the data types of each feature and count the missing values or empty cells of the dataframe. <br>\n",
    "Now we can perform the data wrangling or data cleaning on our dataset and drop the following columns from our dataframe <br>\n",
    "1) feature 'SEVERITYCODE.1' - since its the duplicate feature <br>\n",
    "2) features 'INTKEY', 'EXCEPTRSNCODE', 'EXCEPTRSNDESC', 'INATTENTIONIND', 'PEDROWNOTGRNT', 'SDOTCOLNUM' - Since most of the entries in these columns are empty which cannot be replaced with any other value <br>\n",
    "3) feature 'SPEEDING' - although speeding causes accident but in our case 95% of the entries are nan and also it is not required as per our problem statement <br>\n",
    "4) features 'X', 'Y', 'INCKEY', 'COLDETKEY', 'REPORTNO', 'STATUS', 'ADDRTYPE', 'LOCATION', 'UNDERINFL', 'SEGLANEKEY', 'CROSSWALKKEY', 'HITPARKEDCAR', 'SDOT_COLCODE',\n",
    "'SDOT_COLDESC', 'ST_COLCODE', 'ST_COLDESC' - these additional features can be safely dropped as they are not reuired as per our problem statement <br>\n",
    "5) features 'PERSONCOUNT', 'PEDCOUNT', 'PEDCYLCOUNT', 'INCDATE', 'INCDTTM', 'JUNCTIONTYPE' - these features can also be safely dropped\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "SO after the first initial cleaning process we have reduced the features from 37 to only 7 which we will be using for model prediction. <br>\n",
    "The **Final Dataset** consists of the following features:- <br>\n",
    "1) SEVERITYCODE -INT- Code that corresponds to the severity of the collision<br>\n",
    "2) SEVERITYDESC -TEXT- Detailed description of the severity of the collision<br>\n",
    "3) COLLISIONTYPE -TEXT- The type of Collision<br>\n",
    "4) VEHCOUNT -INT- The number of vehicles involved in the collision<br>\n",
    "5) WEATHER -TEXT- Description of the weather conditions during the time of the collision<br>\n",
    "6) ROADCOND -TEXT- The condition of the road during the collision<br>\n",
    "7) LIGHTCOND -TEXT- The light conditions during the collision<br>\n",
    "\n",
    "\n",
    "On further analysis you will find that there are still some missing values in the above features. Additionally there are some 'unknown' and 'other' values in the dataset. There are two ways to handle these values - <br>\n",
    "1) replace missing or unknown values with avg. value of the feature column but being categorical variable this is not feasible <br>\n",
    "2) replace them with feature value with max. frequency but that might create bias <br>\n",
    "So we will drop all these entries (rows) with missing and unknown values without affecting the dataset as we have many entries. \n",
    " "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## How to perform Modelling on Cleaned Dataset\n",
    "As we now know  that we need to predict the Accident Severity, so SeverityCode is our target variable or dependent variable and all other features are independent variable. The target feature has only two groups (1 & 2) ie. the dataset can be classified as either Injury collision or Damage only collision. This is a supervised learning of type classification where the target feature will be trained and tested against the available dataset. <br>\n",
    "**Classification** is one of the most common machine learning problems. The best way to approach any classification problem is to start by analyzing and exploring the dataset in what we call Exploratory Data Analysis (EDA).<br>\n",
    "In our case we  have an unbalance dataset as the frequency of two groups of Severity Code are not equal. We need to balance this dataset by applying the any of the following techniques -<br>\n",
    "1) Resampling Technique by Undersampling or <br>\n",
    "2) Resampling Technique by Oversampling <br>\n",
    "\n",
    "### Transforming Categorical features\n",
    "Next in the process is to handle Categorical values as in our case all the features are categorical in nature. Many machine learning algorithms cannot support categorical values, so the challenge is to convert them into numerical values. Fortunately, the python tools of pandas and scikit-learn provide several approaches that can be applied to transform the categorical data into suitable numeric values. The various approaches for encoding the Categorical values are - <br>\n",
    "1) Approach-1 (Find & Replace) <br>\n",
    "2) Approach#2 - Label Encoding <br>\n",
    "3) Approach#3 - One Hot Encoding <br>\n",
    "4) Approach#4 - Custom Binary Encoding\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Modelling and Evaluation\n",
    "\n",
    "Once we have transformed the dataset, we can use any of the below methods for Binary Classification of our dataset -<br>\n",
    "A# Decision Tress <br>\n",
    "B# K-Nearest Neighbours <br>\n",
    "C# Support Vector Machines (SVM) <br>\n",
    "D# Naive Bayes <br>\n",
    "E# Logistic Regression <br>\n",
    "\n",
    "The Scikit-Learn package provides all the necesasry functions to carry out the above medelling."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}